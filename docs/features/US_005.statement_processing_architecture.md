# Statement Processing Architecture

This document defines the architecture and responsibilities of the components involved in processing uploaded bank statements in a backend system. The system consists of two main services:

* `StatementAnalyzerService`
* `StatementPersistenceService`

## Goals

The system should:

* Accept file uploads (CSV, XLSX).
* Parse and normalize data into a standard schema.
* Categorize and persist transactions.
* Avoid redundant processing of similar files via hashing.

---

## Component Overview

### 1. `StatementAnalyzerService`

#### Responsibilities

1. **Input**: `filename`, `filecontent`.
2. **Compute file hash** from `filename` + `filecontent` (optional: include file type).
3. **Deduplication**:

   * If hash exists in `UploadedFile` table:

     * Skip reprocessing.
     * Return stored metadata and sample.
   * Else:

     * Use `StatementFileTypeDetector` to detect file type.
     * Use `StatementParser` to read as DataFrame, according to file type.
     * Use `SchemaDetector` to:

       * Detect standard column mapping (e.g., `date`, `amount`, `description`).
       * Identify `header_row_index` and `data_start_row_index`.
     * Use `TransactionNormalizer` to transform and normalize the first 5 data rows.
     * Save all metadata (including normalized sample and hash) to `UploadedFile`.
4. **Return Output**:

```json
{
  "uploaded_file_id": "uuid",
  "file_type": "CSV",
  "column_mapping": {
    "date": "Data",
    "amount": "Valor",
    "description": "Descrição"
}  },
  "header_row_index": 3,
  "data_start_row_index": 4,
  "sample_data": [
    { "date": "...", "amount": "...", "description": "..." },
    ...
  ]
}
```

### 2. `StatementPersistenceService`

#### Responsibilities

1. **Input**: Full metadata from `StatementAnalyzerService`, (excluding sample).
2. Use `StatementParser` again to re-read file based on provided info.
3. Use `TransactionNormalizer` to normalize the full data set.
4. Save valid transactions to the `Transaction` table, associated with the corresponding `UploadedFile`.

---

## Supporting Components

### `StatementFileTypeDetector`
* Receives file content.
* Detects file type (CSV, XLSX).
* Returns file type

### `StatementParser`

* Receives file content and file type.
* Parses content into a DataFrame.
* Returns DataFrame.

### `SchemaDetector`

* Receives raw DataFrame.
* Determines:

  * Column mapping to standard schema.
  * Header row index.
  * Data start index.

Implementation Note: LLM-Backed Schema Detection

The SchemaDetector uses a language model (LLM) to infer schema.

It should accept an injected LLM client via an interface like:

```python
from abc import ABC, abstractmethod

class LLMClient(ABC):
    @abstractmethod
    def complete(self, prompt: str) -> str:
        """
        Sends a prompt to the language model and returns the raw completion text.
        """
        pass
```

The SchemaDetector uses this client to send prompts and parse results.

For example:

```python
class SchemaDetector:
    def __init__(self, llm_client: LLMClient):
        self.llm = llm_client

    def detect_schema(self, df: pd.DataFrame) -> dict:
        prompt = self._build_prompt(df)
        response = self.llm.complete(prompt)
        return self._parse_response(response)
```

Returned schema should include:

Unit tests should mock the LLMClient interface to simulate various LLM outputs.

```python
class MockLLMClient(LLMClient):
    def __init__(self, fixed_response: str):
        self.fixed_response = fixed_response

    def complete(self, prompt: str) -> str:
        return self.fixed_response
```


### `TransactionNormalizer`

* Transforms parsed rows into standard schema:

  * `date`, `amount`, `description`
  * Returns normalized DataFrame.


---

## UploadedFile Table (Suggested Schema)

| Column                 | Type          | Description                           |
| ---------------------- | ------------- | ------------------------------------- |
| `id`                   | UUID          | Primary key                           |
| `created_at`           | Timestamp     | Upload timestamp                      |
| `filename`             | Text          | Original file name                    |
| `file_type`            | Enum          | CSV, XLSX, etc.                       |
| `file_hash`            | Text (unique) | Computed hash from filename + content |
| `column_mapping`       | JSONB         | Mapping to standard columns           |
| `header_row_index`     | Integer       | Index of header row                   |
| `data_start_row_index` | Integer       | Index of first data row               |

---

## Summary

This design:

* Ensures reusable, idempotent processing of uploaded files.
* Cleanly separates concerns between parsing, analyzing, and persisting.
* Provides consistent interfaces for downstream processing or UI interaction.
* Enables optimization by skipping known uploads via hashing.

Next step: implement classes/services based on these contracts and responsibilities.
